<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Karim Haroun</title>
  <link rel="stylesheet" href="styles.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css"> <!-- Font Awesome CDN -->

</head>
<body>
  <header>
    <nav>
      <ul>
        <li><a href="index.html">Home</a></li>
        <li><a href="publications.html">Publications</a></li>
        <li><a href="cv.html">Resume</a></li>
        <li><a href="research.html">Research</a></li>
        <li><a href="contact.html">Contact</a></li>
      </ul>
    </nav>
  </header>

  <main>
    <section class="bio">
      <div class="text-content">
        <h1 style="color: #1E90FF;">Karim Haroun</h1>
        <div class="links">
          <p>
            My research focuses on neural network compression, particularly pruning and quantization techniques. I also have a strong 
            interest in dynamic deep learning during training and how it can be leveraged for continual learning. Additionally, I am 
            exploring various machine learning applications, including EEG signal processing, computer vision, and natural language 
            processing.
          </p>
          <p>
            From 2022 to 2025, I was a PhD student in deep learning and computer vision, affiliated to the Embedded Artificial 
            Intelligence Lab (LIAE) from <a href="https://list.cea.fr/fr/" target="_blank", class="a"> CEA-LIST</a>, and
            the SPARKS team at the Laboratory of Computer Science, 
            Signals and Systems of Sophia Antipolis (<a href="https://www.i3s.unice.fr/en/" target="_blank", class="a">I3S</a>), 
            Université Côte d'Azur. In my thesis, I have worked on dynamic neural network compression, a novel compression paradigm where the computational graph is adapted to the 
            hardness of input instances, which leads to input-adaptive allocation of computation. This contrasts with traditional static compression techniques where the 
            goal is to reduce the size of the model during training, either by pruning weights and/or activations, or by reducing the precision of the weights 
            and/or activations using quantization. Specifically, I was interested in how to leverage attention in Transformer models to guide the inference 
            and design dynamic compression strategies that varies the length of the input sequence by leveraging information redundancies between tokens.
          </p>
          <!-- <p>
            Currently looking for postdoc offers, I am planning to explore the theoretical underpinnings of visual attention and dynamic compression. This concern model uncertainty and the confidence 
            for prediction, this will allow us to 
            
            dynamic inference 
            based on solid theoretical groundings. 

          </p> -->
        </div>

        <div>

        </div>
        
        <div class="links" >
          <!-- <a href="https://www.linkedin.com/in/k-haroun/" target="_blank", class="b"><i class="fa-solid fa-graduation-cap"></i> <strong>Scholar</strong></a> -->
          <a href="https://www.linkedin.com/in/k-haroun/" target="_blank", class="b"><i class="fa-brands fa-linkedin"></i> <strong>Linkedin</strong></a>
          <a href="https://github.com/karimharoun" target="_blank", class="b"><i class="fa-brands fa-github"></i> <strong>GitHub</strong></a>
          <a href="https://orcid.org/0009-0000-6972-6019" target="_blank", class="b" ><i class="fa-brands fa-orcid"></i> <strong>ORCID</strong></a>
          
        </div>
      </div>
      <img src="profile.png" alt="Profile photo of Karim Haroun">
    </section>
  </main>

</body>
</html>
